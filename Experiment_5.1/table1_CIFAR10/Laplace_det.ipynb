{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/LOCAL//anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/LOCAL//anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/LOCAL//anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/LOCAL//anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/LOCAL//anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/LOCAL//anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#### Load Data ####\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.85\n",
    "session = tf.Session(config=config)\n",
    "    \n",
    "def normalize(X_train,X_test):\n",
    "    #this function normalize inputs for zero mean and unit variance\n",
    "    # it is used when training a model.\n",
    "    # Input: training set and test set\n",
    "    # Output: normalized training set and test set according to the trianing set statistics.\n",
    "    mean = np.mean(X_train,axis=(0,1,2,3))\n",
    "    std = np.std(X_train, axis=(0, 1, 2, 3))\n",
    "    X_train = (X_train-mean)/(std+1e-7)\n",
    "    X_test = (X_test-mean)/(std+1e-7)\n",
    "    return X_train, X_test\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train, x_test = normalize(x_train, x_test)\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=False)  # randomly flip images\n",
    "\n",
    "# (std, mean, and principal components if ZCA whitening is applied).\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def get_weight_grad(model, inputs, outputs):\n",
    "    \"\"\" Gets gradient of model for given inputs and outputs for all weights\"\"\"\n",
    "    grads = model.optimizer.get_gradients(model.total_loss, model.trainable_weights)\n",
    "    symb_inputs = (model._feed_inputs + model._feed_targets + model._feed_sample_weights)\n",
    "    f = K.function(symb_inputs, grads)\n",
    "    x, y, sample_weight = model._standardize_user_data(inputs, outputs)\n",
    "    output_grad = f(x + y + sample_weight)\n",
    "    return output_grad\n",
    "\n",
    "def get_layer_output_grad(model, inputs, outputs, layer=-1):\n",
    "    \"\"\" Gets gradient a layer output for given inputs and outputs\"\"\"\n",
    "    grads = model.optimizer.get_gradients(model.total_loss, model.layers[layer].output)\n",
    "    symb_inputs = (model._feed_inputs + model._feed_targets + model._feed_sample_weights)\n",
    "    f = K.function(symb_inputs, grads)\n",
    "    x, y, sample_weight = model._standardize_user_data(inputs, outputs)\n",
    "    output_grad = f(x + y + sample_weight)\n",
    "    return output_grad\n",
    "\n",
    "def tran_a(a_1):\n",
    "    a_1 = np.mean(a_1, axis=0)\n",
    "    a_1 = a_1.reshape(a_1.shape[0]*a_1.shape[1],-1)\n",
    "    a_1 = np.mean(a_1,axis=0)\n",
    "    return a_1\n",
    "    \n",
    "def tran_h(h_2):\n",
    "    h_2 = np.mean(h_2, axis=0)\n",
    "    h_2 = h_2.reshape(h_2.shape[0]*h_2.shape[1],-1)\n",
    "    h_2 = np.mean(h_2,axis=0)+10**-13\n",
    "    return h_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /LOCAL//anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /LOCAL//anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "0.2996946773934364\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "model = keras.models.load_model('vgg16_dropout/final_139.h5',compile=False)\n",
    "sgd = optimizers.SGD_test(add_ah=False, add_noise=0.0, a_corr=np.zeros([12,512,512]), h_corr=np.zeros([12,512,512]), lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
    "\n",
    "trn_loss, trn_acc = model.evaluate(x_train, y_train,verbose=0)\n",
    "tst_loss, tst_acc = model.evaluate(x_test, y_test,verbose=0)\n",
    "print(tst_loss-trn_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropout: -13, -12\n",
    "#normal: -11, -10\n",
    "\n",
    "from keras import optimizers\n",
    "\n",
    "def load(fname, a):\n",
    "    K.clear_session()\n",
    "\n",
    "    model = keras.models.load_model(fname+'/final_139.h5',compile=False)\n",
    "    sgd = optimizers.SGD_test(add_ah=False, add_noise=0.0, a_corr=np.zeros([12,512,512]), h_corr=np.zeros([12,512,512]), lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
    "\n",
    "    a_1 = []\n",
    "    h_2 = []\n",
    "    num = 0\n",
    "    layer_1 = K.function([model.layers[0].input], [model.layers[a].output])\n",
    "    for num in [0,20,40,60,80,100,120]:\n",
    "        a_1.append(tran_a(layer_1([x_train[num:num+20]])[0]))\n",
    "\n",
    "    for num in [0,20,40,60,80,100,120]:\n",
    "        h_2.append(tran_h(get_layer_output_grad(model, x_train[num:num+20], y_train[num:num+20], a+1)[0]))\n",
    "    \n",
    "    return a_1, h_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg16\n",
      "0.04755795554573536\n"
     ]
    }
   ],
   "source": [
    "split = 16\n",
    "\n",
    "for fname in ['vgg16']:\n",
    "    a_1, h_2 = load(fname, -11)\n",
    "    deet_a = []\n",
    "    for i in range(int(len(a_1[0])/split)):\n",
    "        a = np.kron(a_1[0][i*split:i*split+split], np.array([a_1[0][i*split:i*split+split]]).reshape(-1,1))\n",
    "        for j in range(1,len(a_1)):\n",
    "            a += np.kron(a_1[j][i*split:i*split+split], np.array([a_1[j][i*split:i*split+split]]).reshape(-1,1))\n",
    "        a = np.abs(np.linalg.inv(a))\n",
    "        a = a/np.max(a)\n",
    "        for j in range(len(a)):\n",
    "            a[j][j] = 1\n",
    "        deet_a.append(np.linalg.det(a))\n",
    "    deet_a = np.mean(deet_a)\n",
    "\n",
    "    deet_h = []\n",
    "    for e in range(20):\n",
    "        for i in range(int(len(a_1[0])/split)):\n",
    "            a = np.kron(h_2[0][i*split:i*split+split], np.array([h_2[0][i*split:i*split+split]]).reshape(-1,1))  \n",
    "            for j in range(1,len(h_2)):\n",
    "                a += np.kron(h_2[j][i*split:i*split+split], np.array([h_2[j][i*split:i*split+split]]).reshape(-1,1))\n",
    "            a = a/np.max(a)\n",
    "            a += np.random.normal(loc=0.0, scale=0.00001, size=(len(a),len(a)))\n",
    "            a = np.abs(np.linalg.inv(a))\n",
    "            a = a/np.max(a)\n",
    "            for j in range(len(a)):\n",
    "                a[j][j] = 1\n",
    "            deet_h.append(np.linalg.det(a))\n",
    "    deet_h = np.mean(deet_h)     \n",
    "    print(fname)\n",
    "    print(deet_a*deet_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg16_dropout\n",
      "0.13978243277932703\n"
     ]
    }
   ],
   "source": [
    "#dropout module\n",
    "\n",
    "split = 16\n",
    "\n",
    "for fname in ['vgg16_dropout']:\n",
    "    a_1, h_2 = load(fname, -13)\n",
    "    deet_a = []\n",
    "    for i in range(int(len(a_1[0])/split)):\n",
    "        a = np.zeros((split,split))\n",
    "        for e in range(50):\n",
    "            noise = (np.random.uniform(0,1,split)>0.4).astype(float)\n",
    "            a += np.kron(a_1[0][i*split:i*split+split]*noise, np.array([a_1[0][i*split:i*split+split]*noise]).reshape(-1,1))\n",
    "            for j in range(1,len(a_1)):\n",
    "                a += np.kron(a_1[j][i*split:i*split+split]*noise, np.array([a_1[j][i*split:i*split+split]*noise]).reshape(-1,1))    \n",
    "        a = np.abs(np.linalg.inv(a/50))\n",
    "        a = a/np.max(a)\n",
    "        for j in range(len(a)):\n",
    "            a[j][j] = 1\n",
    "        deet_a.append(np.linalg.det(a))\n",
    "    deet_a = np.mean(deet_a)\n",
    "\n",
    "\n",
    "    deet_h = []\n",
    "    for e in range(20):\n",
    "        for i in range(int(len(a_1[0])/split)):\n",
    "            a = np.kron(h_2[0][i*split:i*split+split], np.array([h_2[0][i*split:i*split+split]]).reshape(-1,1))  \n",
    "            for j in range(1,len(h_2)):\n",
    "                a += np.kron(h_2[j][i*split:i*split+split], np.array([h_2[j][i*split:i*split+split]]).reshape(-1,1))\n",
    "            a = a/np.max(a)\n",
    "            a += np.random.normal(loc=0.0, scale=0.00001, size=(len(a),len(a)))\n",
    "            a = np.abs(np.linalg.inv(a))\n",
    "            a = a/np.max(a)\n",
    "            for j in range(len(a)):\n",
    "                a[j][j] = 1\n",
    "            deet_h.append(np.linalg.det(a))\n",
    "    deet_h = np.mean(deet_h)     \n",
    "    print(fname)\n",
    "    print(deet_a*deet_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
